---
title: "Make Location Filtered GPS Data" 
author: "Claire Punturieri"
date: "`r lubridate::today()`"
format: 
  html:
    code-fold: true
    code-summary: "Click to see code" 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---   

### Notes

 - Current filtering follows CP's GPS project decisions and further removes points that are deemed not trustworthy. We might want to play around with/discuss other filtering options at some point.

### Setup

```{r}
#| message: false

options(conflicts.policy = "depends.ok")

library(tidyverse)
library(future)

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/format_path.R?raw=true")

path_processed <- format_path("risk/data_processed/shared")
path_gps <- format_path("risk/data_processed/gps")
path_terrain <- format_path("risk/data_processed/terrain")

dist_max <- 0.031   # only use context if places are within 50 meters (0.031 miles)
```

### Read in files

```{r}
lapses <- read_csv(here::here(path_terrain, "lapses.csv"),
                show_col_types = FALSE) |>
  mutate(lapse_start = with_tz(lapse_start, tz = "America/Chicago"),
         lapse_end = with_tz(lapse_end, tz = "America/Chicago"),
         lapse_id = 1:n()) |> # each lapse gets unique identifier
  glimpse()

lapses <- lapses |>
  group_by(subid) |> 
  mutate(lapse_total = n(), # count total number of lapses per subj
         lapse_no = 1:n()) |>  # label each lapse no per subj
  ungroup()

context <- read_csv(here::here(path_processed, "gps_enriched.csv.xz"), show_col_types = FALSE) |>
  # variable conversions
  mutate(time = with_tz(time, tz = "America/Chicago"),
         dist = dist / 1609.344, # convert to miles
         duration = duration / 60, # convert to hours
         speed = dist / duration,
         dist_context = dist_context / 1609.344) |> # convert to miles
  mutate(duration = if_else(dist > 0.01 & duration == 0, NA_real_,
                            duration),
         duration = if_else(speed > 100, NA_real_,
                            duration),
         duration = if_else(duration > .5 & dist > 0.31, NA_real_,
                            duration),
         known_loc = if_else(dist_context <= 0.031 & speed <= 4,
                             TRUE, FALSE),
         known_loc = if_else(is.na(known_loc), FALSE, known_loc)) |>
  rename(dttm_obs = time) |> 
  select(subid, lat, lon, dttm_obs, dist, duration, context_id, known_loc,
         lat_context, lon_context, dist_context, risk)

# filter out points we've decided we do not trust
context <- context |> drop_na(duration) |> select(-dist)

locations <- read_csv(here::here(path_processed, "locations.csv"), show_col_types = FALSE)
```

### Filter subjects

Filter data down to subjects who had a lapse. Count subjects we are losing and number of total lapses retained.
```{r}
subids_total <- context |>  
  pull(subid) |>  
  unique()

subids_lapses <- lapses |>  
  pull(subid) |>  
  unique()

context <- context |>
  filter(subid %in% subids_lapses)
```
`r length(subids_total)` subjects were in the original data set, `r length(subids_lapses)` subjects had a lapse. In total, there are `r lapses |> nrow()` distinct lapse events.

### Isolate to lapse events

Currently, the GPS data has been filtered to only include subjects who experienced a lapse event. The next step is to filter the GPS data down to *when* a lapse event was occurring.

This function takes in the GPS and lapse information for an individual and filters GPS data down to only GPS points collected during a lapse period. This includes *the last point* prior to when a lapse event started.
```{r}
lapse_filter <- function(id, context, lapses) {
  
  context_tmp <- context |> filter(subid == id)
  context_tmp_id <- context_tmp |> pull(subid) |> unique()
   
  lapses_tmp <- lapses |> filter(subid == id)
  
  lapses_tmp <- lapses_tmp |>
    rowwise() |> 
    mutate(
      last_point = {
        prev_point <- context_tmp |> 
          filter(dttm_obs < lapse_start) |> 
          slice_max(dttm_obs, n = 1)
        
        if (nrow(prev_point) > 0) prev_point$dttm_obs[1] else lapse_start
      }
    )
  
  
  # filter based on shifted start
  context_tmp <- context_tmp |> 
    rowwise() |> 
    mutate(
      matched_lapse = list({
        # get points in the window
        lapses_tmp |>
          filter(dttm_obs >= last_point & dttm_obs <= lapse_end) |> 
        select(last_point, lapse_start, lapse_end, lapse_id, lapse_no, lapse_total)
    })
    ) |> 
    unnest(cols = c(matched_lapse))

  if (nrow(context_tmp) == 0) {
    print(paste0("Subject ", context_tmp_id, " has no GPS retained for at least one lapse!"))
  }
  
  return(context_tmp)
}
```

Apply the function to the GPS data and save out the new filtered version. This function will print out subject IDs who are missing GPS data during at least one of their lapse events.
```{r}
future::plan(multisession, workers = parallel::detectCores(logical = FALSE))

context_filtered <- context$subid |>
  unique() |>
  furrr::future_map(\(subid) lapse_filter(id = subid, context, lapses)) |>  
  list_rbind() |> 
  select(subid, lat, lon, dttm_obs, lapse_start, lapse_end, lapse_id, lapse_no, lapse_total, everything())
```

### Intermediate EDA

Identify number of subjects who have been retained (i.e., have GPS points that occur during a lapse event).
```{r}
nsub <- context |>
  distinct(subid) |>
  nrow()

nsub_filtered <- context_filtered |> 
  distinct(subid) |> 
  nrow()
```
We started with `r nsub` subjects and ended with `r nsub_filtered` subjects.

Identify what subjects are missing from context_filtered and their lapses (this will be empty if we are pulling the last point prior to a lapse window, because then every lapse will have at least one point).
```{r}
subids_filter <- context_filtered |>  
  pull(subid) |>  
  unique()

missing_subjects <- context |>
  filter(!subid %in% subids_filter) |> 
  pull(subid) |> 
  unique()

missing_lapses <- lapses |> 
  filter(subid %in% missing_subjects) |> 
  pull(lapse_id) |> 
  unique()
```
We are losing subjects: `r missing_subjects` with lapse ids: `r missing_lapses`. Each subject who is dropped from the data at this point only had one lapse event while on study **and** had no corresponding GPS data to match that lapse event.

Identify number of unique lapse identifiers that have been retained. Contrary to the subjects who were dropped from the data completely, we will lose some lapse events for certain subjects if there was no GPS data during a lapse event (but we will still retain that subject if they have other lapse events that do have GPS data).
```{r}
nlapse <- lapses |> 
  nrow()

nlapse_filtered <- context_filtered |> 
  distinct(lapse_id) |> 
  nrow()
```
We started with `r nlapse` lapses and ended with `r nlapse_filtered` lapses. This means that `r nlapse - nlapse_filtered` lapse events did not have GPS data at the time of the lapse. These individuals appear to not have GPS data because they did not have reliable GPS during that point on study so it was truncated at an earlier part. This made sense for duration calculations but maybe doesn't make sense here, discuss?

Identify lapse IDs with no GPS data.
```{r}
lapse_ids_filtered <- context_filtered |>  
  pull(lapse_id) |>  
  unique()

missing_lapse_ids <- lapses |> 
  filter(!lapse_id %in% lapse_ids_filtered) |> 
  pull(lapse_id) |> 
  unique()
```

Identify number of observations at which an individual is at a known location.
```{r}
table(context_filtered$known_loc)
```

Of known locations, display relative risk levels.
```{r}
context_filt_known <- context_filtered |>
  filter(known_loc == TRUE)

table(context_filt_known$risk)
```

Of known locations, how do relative risk levels vary **outside** of lapse periods?
```{r}
dttm_filt <- context_filtered |>  
  pull(dttm_obs) |>  
  unique()

context_nolapse <- context |>
  filter(!dttm_obs %in% dttm_filt) |> 
  filter(known_loc == TRUE)

table(context_nolapse$risk)
```

Of known locations, how many observations are missing a risk level? These missing observations do not have a risk level that was identified at interview; therefore, we will drop these observations (two lapses from subid 3 at context_id 49).
```{r}
sum(is.na(context_filt_known$risk))

context_filt_known <- context_filt_known[!is.na(context_filt_known$risk),]
```

Within a lapse event, if there is a point retained prior to the lapse event *after* filtering down to known locations, does this point **match** the next point in the lapse window?
```{r}
context_filt_known |> 
  group_by(lapse_id) |>
  slice_head(n = 2) |> # take first two points
  filter(n() == 2) |> # keep only points that have at least two 
  mutate(same_loc = if_else(
    first(context_id) == last(context_id),
    TRUE,
    FALSE
  )) |>
  select(subid, lapse_id, same_loc) |> 
  unique() |>
  ungroup() |> 
  count(same_loc)
```

Let's also check the amount of time that occurs between the last point before the lapse window and the first point within the lapse window. This might help us make some filtering decisions because we might decide that we want to set a threshold for the amount of time between them (even if someone is at the same location).

Reminders: duration is in hours and is calculated as amount of time SINCE the previous point. Therefore, we want to look at the amount of time between points when there is a point that occurs outside of the lapse window.

Cross-checking high duration places with the locations file, all places that have a duration of greater than 10 hours are all someone's home. Most, but not all, of them are the same location as the previous point. Therefore, I think we could just drop the first point if it's not at the same location and retain if it is (because someone could spent 24 hours at home).
```{r}
context_filt_known |> 
  group_by(lapse_id) |>
  slice_head(n = 2) |> # take first two points
  filter(n() == 2) |> # keep only points that have at least two
  mutate(same_loc = if_else(
    first(context_id) == last(context_id),
    TRUE,
    FALSE
  )) |>
  slice_tail() |> # then we need to further filter down to the second of those two points
  #filter(same_loc == FALSE) |> view()
  pull(duration) |> 
  hist()

context_filt_known |> 
  group_by(lapse_id) |>
  slice_head(n = 2) |>
  filter(n() == 2) |>
  slice_tail() |>
  pull(duration) |> 
  summary()
```

### Final filtering

First step is to create a function that takes an argument to either filter down to modal lat/lon during lapse or during the first hour of a lapse.

```{r}
modal_lapse <- function(data, lapse_col, lapse_no_value, lat_col, lon_col, duration_col, first_hour = FALSE) {
  # filter by lapse event
  tmp <- data |> filter(.data[[lapse_col]] == lapse_no_value)
  
    if (first_hour == TRUE) {
    # filter per row for first-hour window
      tmp <- tmp |> rowwise() |> 
        filter(dttm_obs >= last_point & dttm_obs <= lapse_start + hours(1)) |> 
        ungroup()
    
    # if no points passed, fall back to last_point itself
    if (nrow(tmp) == 0) {
      tmp <- data |> filter(.data[[lapse_col]] == lapse_no_value & dttm_obs == last_point)
    }
  
    # compute modal location
    if (nrow(tmp) == 1) {
      tmp_longest <- tmp
    } else {
      tmp_longest <- tmp |> 
        group_by(.data[[lat_col]], .data[[lon_col]]) |> 
        summarize(duration = sum(.data[[duration_col]]), .groups = "drop") |>
        slice_max(order_by = duration)
    }
  } else {
    tmp_longest <- tmp |> 
      group_by(.data[[lat_col]], .data[[lon_col]]) |> 
      summarize(duration = sum(.data[[duration_col]]), .groups = "drop") |>
      slice_max(order_by = duration)
  }
  
  # returns rows for matching top locations
  result <- tmp |> 
    filter((.data[[lat_col]] %in% tmp_longest[[lat_col]]) &
             (.data[[lon_col]] %in% tmp_longest[[lon_col]])) |>
    mutate(date = as_date(dttm_obs)) |> 
    select(any_of(c("subid", "date", lapse_col, lat_col, lon_col,
                    "lapse_start", "lapse_end",
                    "lapse_no", "lapse_total", "risk")))
}
```

#### Filter to modal GPS point during lapse

```{r}
data_full_lapse <- context_filt_known$lapse_id |>
  unique() |>
  purrr::map(\(lapse_id_value) modal_lapse(data = context_filt_known, lapse_id_value,
                                                   lapse_col = "lapse_id",
                                                   lat_col = "lat", lon_col = "lon",
                                                   duration_col = "duration",
                                           first_hour = FALSE)) |>
  list_rbind() |> 
  distinct(lapse_id, .keep_all = TRUE)
```

#### Filter to modal GPS point during first hour of lapse

```{r}
data_hour_lapse <- context_filt_known$lapse_id |>
  unique() |>
  purrr::map(\(lapse_id_value) modal_lapse(data = context_filt_known, lapse_id_value,
                                                   lapse_col = "lapse_id",
                                                   lat_col = "lat", lon_col = "lon",
                                                   duration_col = "duration",
                                           first_hour = TRUE)) |>
  list_rbind() |> 
  distinct(lapse_id, .keep_all = TRUE)
```

### Final EDA

Show relative risk levels.
```{r}
table(data_full_lapse$risk)
table(data_hour_lapse$risk)
```

What lapses are we losing by only filtering at the hour level?
```{r}
dropped_lapses <- data_full_lapse$lapse_id[!(data_full_lapse$lapse_id %in% data_hour_lapse$lapse_id)]

length(dropped_lapses)
```

### Save out file

```{r}
data_full_lapse |>
  write_csv(here::here(path_terrain, "gps_terrain_full.csv"))

data_hour_lapse |>
  write_csv(here::here(path_terrain, "gps_terrain_hour.csv"))
```


